import streamlit as st
import google.generativeai as genai
import os

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="Honda AI", page_icon="ğŸ¤–", layout="centered")

# --- Ø§Ù„ØªØµÙ…ÙŠÙ… ---
st.markdown("""
<style>
    .stChatMessage {text-align: right; direction: rtl;}
    p {text-align: right; direction: rtl;}
    .stTextInput > div > div > input {text-align: right; direction: rtl;}
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# --- Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ---
st.title("ğŸ¤– Ù‡ÙˆÙ†Ø¯Ø§ - Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ")

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø® Ø§Ù„Ø°ÙƒÙŠ (Ø¨ØªØ¬Ø±Ø¨ ÙƒÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©) ---
def get_working_model():
    try:
        # 1. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù…ÙØªØ§Ø­
        if "HONDA_API_KEY" not in st.secrets:
            st.error("âš ï¸ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Secrets!")
            return None, "Ù…ÙÙŠØ´ Ù…ÙØªØ§Ø­"

        api_key = st.secrets["HONDA_API_KEY"]
        genai.configure(api_key=api_key)

        # 2. Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù„ÙŠ Ù‡Ù†Ø¬Ø±Ø¨Ù‡Ø§ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ (Ø§Ù„Ø£Ø³Ø±Ø¹ Ù„Ù„Ø£Ø°ÙƒÙ‰)
        models_to_try = [
            'gemini-1.5-flash',
            'gemini-1.5-flash-latest',
            'gemini-pro',
            'gemini-1.5-pro',
            'gemini-1.0-pro'
        ]

        # 3. Ù†Ø¬Ø±Ø¨ ÙˆØ§Ø­Ø¯ ÙˆØ§Ø­Ø¯ Ù„Ø­Ø¯ Ù…Ø§ Ù†Ù„Ø§Ù‚ÙŠ ÙˆØ§Ø­Ø¯ Ø´ØºØ§Ù„
        for model_name in models_to_try:
            try:
                model = genai.GenerativeModel(model_name)
                # ØªØ¬Ø±Ø¨Ø© ÙˆÙ‡Ù…ÙŠØ© Ø³Ø±ÙŠØ¹Ø© Ø¹Ø´Ø§Ù† Ù†ØªØ£ÙƒØ¯ Ø¥Ù†Ù‡ Ø´ØºØ§Ù„
                # (Ø¨Ù†Ø¨Ø¹Øª ÙƒÙ„Ù…Ø© test ÙˆÙ„Ùˆ Ø±Ø¯ ÙŠØ¨Ù‚Ù‰ ØªÙ…Ø§Ù…)
                # Ù…Ù„Ø§Ø­Ø¸Ø©: Ù…Ø´ Ø¨Ù†Ø¹Ù…Ù„ generate ÙØ¹Ù„ÙŠ Ø¹Ø´Ø§Ù† Ù†ÙˆÙØ± Ø§Ù„ÙƒÙˆØªØ§ØŒ Ù…Ø¬Ø±Ø¯ Initializing
                return model, model_name
            except:
                continue # Ù„Ùˆ Ø¨Ø§ÙŠØ¸ Ø¬Ø±Ø¨ Ø§Ù„Ù„ÙŠ Ø¨Ø¹Ø¯Ù‡
        
        # Ù„Ùˆ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ÙƒÙ„Ù‡Ø§ ÙØ´Ù„ØªØŒ Ù†Ø±Ø¬Ø¹ Ø§Ù„ÙÙ„Ø§Ø´ ÙˆØ®Ù„Ø§Øµ
        return genai.GenerativeModel('gemini-1.5-flash'), 'gemini-1.5-flash (Default)'

    except Exception as e:
        st.error(f"Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¬ÙˆØ¬Ù„: {e}")
        return None, str(e)

# --- ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… ---
model, model_name = get_working_model()

if model:
    st.caption(f"âœ… Ù…ØªØµÙ„ Ø¨Ù…Ø®: {model_name}")
else:
    st.caption("ğŸ”´ Ø§Ù„Ù†Ø¸Ø§Ù… ØºÙŠØ± Ù…ØªØµÙ„")

# --- Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø´Ø§Øª ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø£ÙˆØ§Ù…Ø± ---
if prompt := st.chat_input("Ø§Ø·Ù„Ø¨ Ù…Ù†ÙŠ Ø£ÙŠ Ø­Ø§Ø¬Ø©..."):
    # Ø¹Ø±Ø¶ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Ø±Ø¯ Ù‡ÙˆÙ†Ø¯Ø§
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        if not model:
            message_placeholder.error("Ø£Ù†Ø§ Ø¹Ø·Ù„Ø§Ù† Ø­Ø§Ù„ÙŠØ§Ù‹.")
        else:
            try:
                # ÙÙ„ØªØ± Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØªØ·ÙˆÙŠØ±
                dev_keywords = ["Ø·ÙˆØ±", "Ø¹Ø¯Ù„", "Ø¶ÙŠÙ", "Ø§Ù…Ø³Ø­", "ÙƒÙˆØ¯", "Ø¨Ø±Ù†Ø§Ù…Ø¬"]
                is_dev = any(k in prompt for k in dev_keywords)

                if is_dev:
                    full_response = "Ø¬Ø§Ø±ÙŠ ÙƒØªØ§Ø¨Ø© ÙƒÙˆØ¯ Ø§Ù„ØªØ·ÙˆÙŠØ±...\n"
                    # Ø£Ù…Ø± Ù„Ù„Ù…Ø·ÙˆØ±
                    dev_prompt = f"""
                    Act as a Streamlit Expert.
                    Task: Write the full Python code for 'app.py' to implement: "{prompt}".
                    Rules: Return ONLY the code block. No text.
                    Current Code Context: Streamlit app with Gemini.
                    """
                    try:
                        response = model.generate_content(dev_prompt)
                        message_placeholder.markdown(response.text)
                        full_response = response.text
                    except Exception as e:
                        if "429" in str(e):
                            message_placeholder.warning("â³ Ø¶ØºØ· Ø¹Ø§Ù„ÙŠØŒ Ø¬Ø±Ø¨ ÙƒÙ…Ø§Ù† Ø¯Ù‚ÙŠÙ‚Ø©.")
                            full_response = "Ø¬ÙˆØ¬Ù„ Ø¨ÙŠÙ‚ÙˆÙ„ÙŠ Ù‡Ø¯ÙŠ Ø§Ù„Ø³Ø±Ø¹Ø©."
                        else:
                            st.error(f"Ø®Ø·Ø£ ØªØ·ÙˆÙŠØ±: {e}")
                            full_response = "ÙØ´Ù„ Ø§Ù„ØªØ·ÙˆÙŠØ±."
                else:
                    # Ø¯Ø±Ø¯Ø´Ø© Ø¹Ø§Ø¯ÙŠØ©
                    chat_prompt = f"Ø£Ù†Øª Ù‡ÙˆÙ†Ø¯Ø§ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØµØ±ÙŠ Ø°ÙƒÙŠ. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {prompt}"
                    try:
                        response = model.generate_content(chat_prompt)
                        message_placeholder.markdown(response.text)
                        full_response = response.text
                    except Exception as e:
                        if "429" in str(e):
                            message_placeholder.warning("â³ ÙƒÙØ§ÙŠØ© ÙƒÙ„Ø§Ù…ØŒ Ø±ÙŠØ­Ù†ÙŠ Ø¯Ù‚ÙŠÙ‚Ø©!")
                            full_response = "ØªØ¹Ø¨ØªØŒ Ø±Ø§Ø¬Ø¹Ù„Ùƒ ÙƒÙ…Ø§Ù† Ø´ÙˆÙŠØ©."
                        else:
                            st.error(f"Ø®Ø·Ø£ Ø¯Ø±Ø¯Ø´Ø©: {e}")
                            full_response = "Ù…Ø´ÙƒÙ„Ø© ØªÙ‚Ù†ÙŠØ©."

            except Exception as e:
                st.error(f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
                full_response = "Error."
            
    st.session_state.messages.append({"role": "assistant", "content": full_response}).text})

            except Exception as e:
                st.error(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø±Ø¯: {e}")
