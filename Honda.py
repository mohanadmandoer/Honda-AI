import streamlit as st
import google.generativeai as genai
import os

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="Honda AI", page_icon="ğŸ¤–", layout="centered")

# --- Ø§Ù„ØªØµÙ…ÙŠÙ… ---
st.markdown("""
<style>
    .stChatMessage {text-align: right; direction: rtl;}
    p {text-align: right; direction: rtl;}
    .stTextInput > div > div > input {text-align: right; direction: rtl;}
</style>
""", unsafe_allow_html=True)

# --- Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ---
st.title("ğŸ¤– Ù‡ÙˆÙ†Ø¯Ø§ - Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ")

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø°ÙƒÙŠ (Ø§Ù„Ø­Ù„ Ø§Ù„Ø³Ø­Ø±ÙŠ) ---
def get_auto_model():
    try:
        if "HONDA_API_KEY" not in st.secrets:
            st.error("âš ï¸ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Secrets!")
            return None, "No Key"

        api_key = st.secrets["HONDA_API_KEY"]
        genai.configure(api_key=api_key)
        
        # 1. Ù†Ø³Ø£Ù„ Ø¬ÙˆØ¬Ù„: Ø¥ÙŠÙ‡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù„ÙŠ Ø´ØºØ§Ù„Ø©ØŸ
        available_models = []
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                available_models.append(m.name)
        
        if not available_models:
            st.error("âŒ Ø§Ù„Ù…ÙØªØ§Ø­ Ø³Ù„ÙŠÙ… Ø¨Ø³ Ù…ÙÙŠØ´ Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ù…ØªØ§Ø­Ø© Ù„Ù„Ø­Ø³Ø§Ø¨ Ø¯Ù‡!")
            return None, "No Models"

     # 2. Ù†Ø®ØªØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ (ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø§Ù…: Ø¨Ù†Ø¬Ø¨Ø±Ù‡ ÙŠØ®ØªØ§Ø± Ø§Ù„ÙÙ„Ø§Ø´ Ø¹Ø´Ø§Ù† Ø§Ù„ÙƒÙˆØªØ§)
        # Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¯ÙŠ Ù…Ø±ØªØ¨Ø© Ù…Ù† Ø§Ù„Ø£Ø³Ø±Ø¹ ÙˆØ§Ù„Ø£ÙˆÙØ± Ù„Ù„Ø£Ø«Ù‚Ù„
        preferences = [
            'models/gemini-2.5-flash',
            'models/gemini-2.5-flash-latest',
            'models/gemini-2.5-flash-001',
            'models/gemini-2.5-pro',
            'models/gemini-2-flash',
            'models/gemini-2-flash-latest',
            'models/gemini-2-flash-001',
            'models/gemini-2-pro',
            'models/gemini-3-flash',
            'models/gemini-3-flash-latest',
            'models/gemini-3-flash-001',
            'models/gemini-3-pro',
            'models/gemini-1.5-flash',
            'models/gemini-1.5-flash-latest',
            'models/gemini-1.5-flash-001',
            'models/gemini-1.5-pro',
            'models/gemini-pro'
        ]
        
        selected_name = None
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙÙ„Ø§Ø´ Ø£ÙˆÙ„Ø§Ù‹
        for pref in preferences:
            if pref in available_models:
                selected_name = pref
                break
        
        # Ù„Ùˆ Ù…Ù„Ù‚Ø§Ø´ ÙˆÙ„Ø§ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ù„ÙŠ ÙÙˆÙ‚ØŒ Ø®Ø¯ Ø£ÙŠ ÙˆØ§Ø­Ø¯ Ù…ØªØ§Ø­ ÙˆØ®Ù„Ø§Øµ
        if not selected_name:
            selected_name = available_models[0]
        
        return genai.GenerativeModel(selected_name), selected_name

    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¬ÙˆØ¬Ù„: {e}")
        return None, str(e)

# --- ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø® ---
model, model_name = get_auto_model()

if model:
    st.caption(f"âœ… ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„: {model_name}")
else:
    st.caption("ğŸ”´ Ø§Ù„Ù†Ø¸Ø§Ù… ØºÙŠØ± Ù…ØªØµÙ„")

# --- Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø´Ø§Øª ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ø§Ø·Ù„Ø¨ Ù…Ù†ÙŠ Ø£ÙŠ Ø­Ø§Ø¬Ø©..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        if not model:
            message_placeholder.markdown("Ø£Ù†Ø§ Ø¹Ø·Ù„Ø§Ù† Ø­Ø§Ù„ÙŠØ§Ù‹.")
        else:
            try:
                # Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØªØ·ÙˆÙŠØ±
                if "Ø·ÙˆØ± Ù†ÙØ³Ùƒ" in prompt or "Ø§ÙƒØªØ¨ ÙƒÙˆØ¯" in prompt:
                    full_response = "Ø¬Ø§Ø±ÙŠ ÙƒØªØ§Ø¨Ø© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯...\n"
                    ai_prompt = f"Ø£Ù†Øª Ø®Ø¨ÙŠØ± Streamlit. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ±ÙŠØ¯: {prompt}. Ø§ÙƒØªØ¨ ÙƒÙˆØ¯ python ÙƒØ§Ù…Ù„ Ù„Ù…Ù„Ù app.py."
                    response = model.generate_content(ai_prompt)
                    message_placeholder.markdown(response.text)
                    full_response = response.text
                else:
                    # Ø¯Ø±Ø¯Ø´Ø© Ø¹Ø§Ø¯ÙŠØ©
                    chat_prompt = f"Ø£Ù†Øª Ù‡ÙˆÙ†Ø¯Ø§ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØµØ±ÙŠ Ø°ÙƒÙŠ. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {prompt}"
                    response = model.generate_content(chat_prompt)
                    message_placeholder.markdown(response.text)
                    full_response = response.text
            except Exception as e:
                st.error(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø±Ø¯: {e}")
                full_response = "Ø­ØµÙ„Øª Ù…Ø´ÙƒÙ„Ø©."
            
    st.session_state.messages.append({"role": "assistant", "content": full_response})
