import streamlit as st
import google.generativeai as genai
import os
import sys

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="Honda AI", page_icon="ğŸ¤–", layout="centered")

# --- Ø§Ù„ØªØµÙ…ÙŠÙ… ---
st.markdown("""
<style>
    .stChatMessage {text-align: right; direction: rtl;}
    p {text-align: right; direction: rtl;}
    .stTextInput > div > div > input {text-align: right; direction: rtl;}
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# --- Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ---
st.title("ğŸ¤– Ù‡ÙˆÙ†Ø¯Ø§ - Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ")

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø® Ø§Ù„Ø°ÙƒÙŠ (Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø°Ø§ØªÙŠ + Ù‚Ø§Ø¦Ù…ØªÙƒ Ø§Ù„Ø®Ø§ØµØ©) ---
def get_working_model():
    try:
        # 1. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù…ÙØªØ§Ø­
        if "HONDA_API_KEY" not in st.secrets:
            st.error("âš ï¸ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Secrets!")
            return None, "Ù…ÙÙŠØ´ Ù…ÙØªØ§Ø­"

        api_key = st.secrets["HONDA_API_KEY"]
        genai.configure(api_key=api_key)

        # 2. Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© (Ø²ÙŠ Ù…Ø§ Ø·Ù„Ø¨Øª Ø¨Ø§Ù„Ø¸Ø¨Ø·)
        # Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‡ÙŠØ¬Ø±Ø¨Ù‡Ù… ÙˆØ§Ø­Ø¯ ÙˆØ§Ø­Ø¯ØŒ ÙˆØ§Ù„Ù„ÙŠ ÙŠØ´ØªØºÙ„ ÙŠÙ…Ø³Ùƒ ÙÙŠÙ‡
        models_to_try = [
            'gemini-2.5-flash',
            'gemini-2.5-flash-latest',
            'gemini-2.5-flash-001',
            'gemini-2.5-pro',
            'gemini-2-flash',
            'gemini-2-flash-latest',
            'gemini-2-flash-001',
            'gemini-2-pro',
            'gemini-3-flash',
            'gemini-3-flash-latest',
            'gemini-3-flash-001',
            'gemini-3-pro',
            'gemini-1.5-flash',
            'gemini-1.5-flash-latest',
            'gemini-1.5-flash-001',
            'gemini-1.5-pro',
            'gemini-pro',
            # --- ØµÙŠØº Ø¨Ø¯ÙŠÙ„Ø© ---
            'models/gemini-1.5-flash',
            'models/gemini-pro'
        ]

        # 3. Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¬Ø±Ø¨Ø© (Loop)
        for model_name in models_to_try:
            try:
                model = genai.GenerativeModel(model_name)
                # Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ (Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø±ØµÙŠØ¯)
                return model, model_name
            except:
                continue # Ù„Ùˆ Ø¨Ø§ÙŠØ¸ØŒ Ø®Ø´ Ø¹Ù„Ù‰ Ø§Ù„Ù„ÙŠ Ø¨Ø¹Ø¯Ù‡
        
        # Ù„Ùˆ ÙƒÙ„Ù‡ ÙØ´Ù„ØŒ Ø±Ø¬Ø¹ Ø§Ù„ÙÙ„Ø§Ø´ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
        return genai.GenerativeModel('gemini-1.5-flash'), 'gemini-1.5-flash (Fallback)'

    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¬ÙˆØ¬Ù„: {e}")
        return None, str(e)

# --- ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… ---
model, model_name = get_working_model()

# Ø¹Ø±Ø¶ Ø­Ø§Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ (Ù„Ù„ØªØ£ÙƒØ¯ ÙÙ‚Ø·)
if model:
    st.caption(f"âœ… Ù…ØªØµÙ„ Ø­Ø§Ù„ÙŠØ§Ù‹ Ø¨Ù…Ø®: {model_name}")
else:
    st.caption("ğŸ”´ Ø§Ù„Ù†Ø¸Ø§Ù… ØºÙŠØ± Ù…ØªØµÙ„")

# --- Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø´Ø§Øª ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø£ÙˆØ§Ù…Ø± ---
if prompt := st.chat_input("Ø£Ù…Ø±Ùƒ ÙŠØ§ Ø²Ø¹ÙŠÙ…..."):
    # Ø¹Ø±Ø¶ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Ø±Ø¯ Ù‡ÙˆÙ†Ø¯Ø§
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        if not model:
            message_placeholder.error("Ø£Ù†Ø§ Ø¹Ø·Ù„Ø§Ù† Ø­Ø§Ù„ÙŠØ§Ù‹. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù…ÙØªØ§Ø­.")
            full_response = "Error: No Model"
        else:
            try:
                # --- Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø®Ø·ÙŠØ±: Ø§Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠ ---
                # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù„ÙŠ Ø¨ØªØ®Ù„ÙŠ Ù‡ÙˆÙ†Ø¯Ø§ ÙŠÙƒØªØ¨ ÙƒÙˆØ¯ Ù„Ù†ÙØ³Ù‡
                dev_keywords = ["Ø·ÙˆØ±", "Ø¹Ø¯Ù„", "Ø¶ÙŠÙ", "Ø§Ù…Ø³Ø­", "ÙƒÙˆØ¯", "Ø¨Ø±Ù†Ø§Ù…Ø¬", "Ø²Ø±Ø§Ø±", "Ø®Ø§ØµÙŠØ©"]
                is_dev = any(k in prompt for k in dev_keywords)

                if is_dev:
                    message_placeholder.warning("âš™ï¸ Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§ØªÙŠ ÙˆØªØ·ÙˆÙŠØ± Ø§Ù„ÙƒÙˆØ¯... Ù„Ø­Ø¸Ø© ÙˆØ§Ø­Ø¯Ø©")
                    
                    # 1. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ (Ø¹Ø´Ø§Ù† ÙŠØ¹Ø±Ù ÙŠØ¹Ø¯Ù„ Ø¹Ù„ÙŠÙ‡)
                    try:
                        current_file = __file__
                        with open(current_file, "r", encoding="utf-8") as f:
                            old_code = f.read()
                    except:
                        # Fallback for some cloud environments
                        current_file = "Honda.py" 
                        old_code = "# Error reading file"

                    # 2. Ø£Ù…Ø± Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØµØ§Ø±Ù… (System Prompt)
                    dev_prompt = f"""
                    ROLE: You are an expert Python Streamlit Developer.
                    TASK: Rewrite the provided code to implement this request: "{prompt}".
                    
                    CURRENT CODE:
                    ```python
                    {old_code}
                    ```
                    
                    RULES:
                    1. Return ONLY the FULL VALID PYTHON CODE. No explanations, no markdown.
                    2. YOU MUST KEEP the 'get_working_model' function and the 'models_to_try' list EXACTLY as they are.
                    3. Ensure correct indentation.
                    4. Do not remove 'import streamlit' or 'api_key' logic.
                    """
                    
                    try:
                        # Ø·Ù„Ø¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡
                        response = model.generate_content(dev_prompt)
                        new_code = response.text.replace("```python", "").replace("```", "").strip()
                        
                        # 3. Ø§Ù„ØªØ­Ù‚Ù‚ ÙˆØ§Ù„Ø­ÙØ¸ (Overwrite)
                        if "import streamlit" in new_code and len(new_code) > 500:
                            with open(current_file, "w", encoding="utf-8") as f:
                                f.write(new_code)
                            
                            message_placeholder.success("âœ… ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨Ù†Ø¬Ø§Ø­! Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ØºÙŠÙ„...")
                            st.session_state.messages.append({"role": "assistant", "content": "ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ø¸Ø§Ù…."})
                            st.rerun() # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙÙˆØ±Ø§Ù‹ Ø¨Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯
                        else:
                            message_placeholder.error("ÙØ´Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ·ÙˆÙŠØ±: Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ø§ØªØ¬ ØºÙŠØ± Ø³Ù„ÙŠÙ….")
                            full_response = "ÙØ´Ù„ Ø§Ù„ØªØ·ÙˆÙŠØ±."

                    except Exception as e:
                        if "429" in str(e):
                            message_placeholder.warning("â³ Ø¶ØºØ· Ø¹Ø§Ù„ÙŠØŒ Ø¬ÙˆØ¬Ù„ Ø¨ÙŠÙ‚ÙˆÙ„ Ø§Ø³ØªÙ†Ù‰ Ø¯Ù‚ÙŠÙ‚Ø©.")
                            full_response = "ØªÙˆÙ‚Ù Ù…Ø¤Ù‚Øª Ù„Ù„Ø±Ø§Ø­Ø©."
                        else:
                            st.error(f"Ø®Ø·Ø£ Ø¨Ø±Ù…Ø¬ÙŠ: {e}")
                            full_response = "ÙØ´Ù„."
                
                else:
                    # --- Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø¹Ø§Ø¯ÙŠ: Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ---
                    chat_prompt = f"Ø£Ù†Øª Ù‡ÙˆÙ†Ø¯Ø§ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØµØ±ÙŠ Ø°ÙƒÙŠ ÙˆÙ…Ø±Ø­. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {prompt}"
                    response = model.generate_content(chat_prompt)
                    message_placeholder.markdown(response.text)
                    full_response = response.text

            except Exception as e:
                st.error(f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
                full_response = "Error."
            
        # Ø­ÙØ¸ Ø§Ù„Ø±Ø¯ (Ù„Ùˆ Ù…ÙƒÙ†Ø´ Ø¹Ù…Ù„ÙŠØ© ØªØ·ÙˆÙŠØ±)
        if not is_dev:
            st.session_state.messages.append({"role": "assistant", "content": full_response})
