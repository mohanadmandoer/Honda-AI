import streamlit as st
import google.generativeai as genai
import os

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="Honda AI", page_icon="ğŸ¤–", layout="centered")

# --- Ø§Ù„ØªØµÙ…ÙŠÙ… ---
st.markdown("""
<style>
    .stChatMessage {text-align: right; direction: rtl;}
    p {text-align: right; direction: rtl;}
    .stTextInput > div > div > input {text-align: right; direction: rtl;}
</style>
""", unsafe_allow_html=True)

# --- Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ---
st.title("ğŸ¤– Ù‡ÙˆÙ†Ø¯Ø§ - Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ")
st.caption("Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø¹Ø§Ùƒ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©ØŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ø°ÙƒØ§Ø¡ Gemini")

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ---
try:
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ù† Ø£Ø³Ø±Ø§Ø± Ø§Ù„Ù…ÙˆÙ‚Ø¹
    if "HONDA_API_KEY" in st.secrets:
        api_key = st.secrets["HONDA_API_KEY"]
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
    else:
        st.error("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØªØ§Ø­ ÙÙŠ Secrets. ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¶Ø§ÙØªÙ‡ Ø¨Ø§Ø³Ù… HONDA_API_KEY")
except Exception as e:
    st.error(f"Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙØªØ§Ø­: {e}")

# --- Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø´Ø§Øª ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- Ø¹Ø±Ø¶ Ø§Ù„Ø´Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ… ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Ø§Ù„ØªÙØ§Ø¹Ù„ ---
if prompt := st.chat_input("Ø§Ø·Ù„Ø¨ Ù…Ù†ÙŠ Ø£ÙŠ Ø­Ø§Ø¬Ø© ÙŠØ§ Ø²Ø¹ÙŠÙ…..."):
    # Ø¹Ø±Ø¶ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ØªÙÙƒÙŠØ± Ù‡ÙˆÙ†Ø¯Ø§
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ø®Ø§ØµØ© (Ø§Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠ)
        if "Ø·ÙˆØ± Ù†ÙØ³Ùƒ" in prompt or "Ø§ÙƒØªØ¨ ÙƒÙˆØ¯" in prompt:
            full_response = "Ø¬Ø§Ø±ÙŠ ÙƒØªØ§Ø¨Ø© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…ÙˆÙ‚Ø¹...\n"
            ai_prompt = f"Ø£Ù†Øª Ø®Ø¨ÙŠØ± Streamlit. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ±ÙŠØ¯: {prompt}. Ø§ÙƒØªØ¨ ÙƒÙˆØ¯ python ÙƒØ§Ù…Ù„ Ù„Ù…Ù„Ù app.py ÙŠØ­Ù‚Ù‚ Ù‡Ø°Ø§."
            try:
                response = model.generate_content(ai_prompt)
                ai_text = response.text
                full_response += ai_text
                message_placeholder.markdown(full_response)
            except Exception as e:
                st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ·ÙˆÙŠØ±: {e}")
                message_placeholder.markdown("Ø¹Ù‚Ù„ÙŠ Ù…Ø´ØºÙˆÙ„ Ø¯Ù„ÙˆÙ‚ØªÙŠ.")
        else:
            # Ø¯Ø±Ø¯Ø´Ø© Ø¹Ø§Ø¯ÙŠØ©
            try:
                chat_prompt = f"Ø£Ù†Øª Ù‡ÙˆÙ†Ø¯Ø§ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø´Ø®ØµÙŠ Ù…ØµØ±ÙŠ. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {prompt}"
                response = model.generate_content(chat_prompt)
                full_response = response.text
                message_placeholder.markdown(full_response)
            except Exception as e:
                # ÙƒØ´Ù Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù‡Ù†Ø§
                st.error(f"âš ï¸ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø®Ø·Ø£: {e}")
                full_response = "Ø¹Ù†Ø¯ÙŠ Ù…Ø´ÙƒÙ„Ø© ØªÙ‚Ù†ÙŠØ©ØŒ Ø¨Øµ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ù…Ø±Ø§Ø¡ ÙÙˆÙ‚."
                message_placeholder.markdown(full_response)
                
    st.session_state.messages.append({"role": "assistant", "content": full_response})
