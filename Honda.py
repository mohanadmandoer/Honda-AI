import streamlit as st
import google.generativeai as genai
import os

    def match = research (pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø® Ø§Ù„Ø°ÙƒÙŠ (Ø¨Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©) ---
def get_working_model():
    try:
        if "HONDA_API_KEY" not in st.secrets:
            st.error("âš ï¸ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Secrets!")
            return None, "No Key"

        api_key = st.secrets["HONDA_API_KEY"]
        genai.configure(api_key=api_key)

        # 2. Ù†Ø®ØªØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ (Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©)
        # Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¯ÙŠ Ù…Ø±ØªØ¨Ø© Ù…Ù† Ø§Ù„Ø£Ø³Ø±Ø¹ ÙˆØ§Ù„Ø£ÙˆÙØ± Ù„Ù„Ø£Ø«Ù‚Ù„
        models_to_try = [
            'gemini-2.5-flash',
            'gemini-2.5-flash-latest',
            'gemini-2.5-flash-001',
            'gemini-2.5-pro',
            'gemini-2-flash',
            'gemini-2-flash-latest',
            'gemini-2-flash-001',
            'gemini-2-pro',
            'gemini-3-flash',
            'gemini-3-flash-latest',
            'gemini-3-flash-001',
            'gemini-3-pro',
            'models/gemini-1.5-flash',
            'models/gemini-1.5-flash-latest',
            'models/gemini-1.5-flash-001',
            'models/gemini-1.5-pro',
            'models/gemini-pro',
            'gemini-1.5-flash',
            'gemini-1.5-pro',
            'gemini-pro'
        ]

        # 3. Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¬Ø±Ø¨Ø© (Loop)
        for model_name in models_to_try:
            try:
                model = genai.GenerativeModel(model_name)
                # Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ (Initializing)
                return model, model_name
            except:
                continue 
        
        # Ù„Ùˆ ÙƒÙ„Ù‡ ÙØ´Ù„ØŒ Ø±Ø¬Ø¹ Ø§Ù„ÙÙ„Ø§Ø´ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
        return genai.GenerativeModel('gemini-1.5-flash'), 'gemini-1.5-flash (Fallback)'

    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¬ÙˆØ¬Ù„: {e}")
        return None, str(e)

# --- ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… ---
model, model_name = get_working_model()

# Ø¹Ø±Ø¶ Ø­Ø§Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„
if model:
    st.caption(f"âœ… Ù…ØªØµÙ„ Ø¨Ù…Ø®: {model_name}")
else:
    st.caption("ğŸ”´ Ø§Ù„Ù†Ø¸Ø§Ù… ØºÙŠØ± Ù…ØªØµÙ„")

# --- Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø´Ø§Øª ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø£ÙˆØ§Ù…Ø± ---
if prompt := st.chat_input("Ø£Ù…Ø±Ùƒ ÙŠØ§ Ø²Ø¹ÙŠÙ…..."):
    # Ø¹Ø±Ø¶ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Ø±Ø¯ Ù‡ÙˆÙ†Ø¯Ø§
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        if not model:
            message_placeholder.error("Ø£Ù†Ø§ Ø¹Ø·Ù„Ø§Ù† Ø­Ø§Ù„ÙŠØ§Ù‹.")
            full_response = "Error: No Model"
        else:
            try:
                # --- Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø®Ø·ÙŠØ±: Ø§Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠ ---
                dev_keywords = ["Ø·ÙˆØ±", "Ø¹Ø¯Ù„", "Ø¶ÙŠÙ", "Ø§Ù…Ø³Ø­", "ÙƒÙˆØ¯", "Ø¨Ø±Ù†Ø§Ù…Ø¬", "Ø²Ø±Ø§Ø±", "Ø®Ø§ØµÙŠØ©", "ÙÙŠØ¯ÙŠÙˆ", "ØµÙˆØ±Ø©"]
                is_dev = any(k in prompt for k in dev_keywords)

                if is_dev:
                    message_placeholder.warning("âš™ï¸ Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙƒÙˆØ¯ ÙˆØªØ·ÙˆÙŠØ± Ø§Ù„Ù†Ø¸Ø§Ù…... (Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ØºÙ„Ù‚)")
                    
                    # 1. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ
                    current_file = __file__
                    try:
                        with open(current_file, "r", encoding="utf-8") as f:
                            old_code = f.read()
                    except:
                        old_code = ""

                    # 2. Ø£Ù…Ø± Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØµØ§Ø±Ù…
                    dev_prompt = f"""
                    Act as an expert Python Streamlit Developer.
                    TASK: Rewrite the ENTIRE current code to implement this request: "{prompt}".
                    
                    CURRENT CODE:
                    ```python
                    {old_code}
                    ```
                    
                    CRITICAL RULES:
                    1. Return the FULL VALID PYTHON CODE only.
                    2. DO NOT include markdown backticks (```) in the start or end if possible.
                    3. KEEP the 'get_working_model' function and the 'models_to_try' list EXACTLY as they are (do not delete the future models).
                    4. KEEP the 'clean_code_block' function.
                    5. Ensure correct indentation.
                    """
                    
                    try:
                        # Ø·Ù„Ø¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯
                        response = model.generate_content(dev_prompt)
                        raw_code = response.text
                        
                        # 3. ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙˆØ¯ (Ø£Ù‡Ù… Ø®Ø·ÙˆØ© Ù„Ù…Ù†Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡)
                        new_code = clean_code_block(raw_code)
                        
                        # 4. Ø§Ù„ØªØ­Ù‚Ù‚ ÙˆØ§Ù„Ø­ÙØ¸
                        if "import streamlit" in new_code and len(new_code) > 500:
                            # Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙˆÙ‚ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„ÙŠ
                            with open(current_file, "w", encoding="utf-8") as f:
                                f.write(new_code)
                            
                            message_placeholder.success("âœ… ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ±! Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ØºÙŠÙ„...")
                            st.session_state.messages.append({"role": "assistant", "content": "ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø¬Ø§Ø­."})
                            st.rerun() # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ ÙÙˆØ±ÙŠØ©
                        else:
                            message_placeholder.error("ÙØ´Ù„ Ø§Ù„ØªØ·ÙˆÙŠØ±: Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ø§ØªØ¬ ØºÙŠØ± Ø³Ù„ÙŠÙ….")
                            # st.code(new_code) # Ø§Ø¹Ø±Ø¶Ù‡ Ù„Ù„Ø¯ÙŠØ¨Ø§Ø¬ Ù„Ùˆ Ø­Ø¨ÙŠØª
                            full_response = "ÙØ´Ù„."

                    except Exception as e:
                        if "429" in str(e):
                            message_placeholder.warning("â³ Ø¶ØºØ· Ø¹Ø§Ù„ÙŠØŒ Ø§Ø³ØªÙ†Ù‰ Ø¯Ù‚ÙŠÙ‚Ø©.")
                            full_response = "ØªÙˆÙ‚Ù Ù…Ø¤Ù‚Øª."
                        else:
                            st.error(f"Ø®Ø·Ø£ Ø¨Ø±Ù…Ø¬ÙŠ: {e}")
                            full_response = "ÙØ´Ù„."
                
                else:
                    # --- Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø¹Ø§Ø¯ÙŠ: Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ---
                    chat_prompt = f"Ø£Ù†Øª Ù‡ÙˆÙ†Ø¯Ø§ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØµØ±ÙŠ Ø°ÙƒÙŠ. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {prompt}"
                    response = model.generate_content(chat_prompt)
                    message_placeholder.markdown(response.text)
                    full_response = response.text

            except Exception as e:
                st.error(f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
                full_response = "Error."
            
        # Ø­ÙØ¸ Ø§Ù„Ø±Ø¯
        if not is_dev:
            st.session_state.messages.append({"role": "assistant", "content": full_response})
